\begin{table}[ht]
  \small 
  \centering
  \caption{10-fold cross-validation classification accuracies of models based
    on flexible frames ($aX + Xb$) and substitutes ($a*b$) on each child corpus
    after 10K training patterns are summarized.  Standard errors are reported
    in parentheses.  Lambdas of $aX+Xb$ and $a*b$ are both tested against each
    other and zero association by using z-test.  All tests have $p<.001$.}
\begin{tabular}{lccccc}
    \hline
    Corpus & \multicolumn{2}{c}{$aX+Xb$} && \multicolumn{2}{c}{$a*b$} \\
    \cline{2-3}
    \cline{5-6}
    & Accuracy & $\lambda$ && Accuracy & $\lambda$\\
    \hline
%% With z-score differences    
%%    & Accuracy & $\lambda$ && Accuracy & $\lambda$ & $\lambda_{a*b}-\lambda_{aX+Xb}$\\
%%    \hline 
%%    Anne  & .6587 (.0255) & .4486 (.0365) && .7945 (.0126) & .6684 (.0301) & 6.02\\
%%    Aran  & .5618 (.0636) & .3332 (.0613) && .7814 (.0099) & .6500 (.0170) & 5.16\\
%%    Eve   & .6360 (.0230) & .4351 (.0359) && .8148 (.0095) & .7127 (.0142) & 7.73\\
%%    Naomi & .6191 (.0322) & .3907 (.0547) && .7917 (.0182) & .6673 (.0290) & 5.06\\
%%    Nina  & .6745 (.0245) & .4818 (.0384) && .8263 (.0124) & .7243 (.0188) & 6.31\\
%%    Peter & .6687 (.0238) & .4857 (.0293) && .8135 (.0139) & .7092 (.0209) & 7.62\\
    Anne  & .6587 (.0255) & .4486 (.0365) && .7945 (.0126) & .6684 (.0301)\\
    Aran  & .5618 (.0636) & .3332 (.0613) && .7814 (.0099) & .6500 (.0170)\\
    Eve   & .6360 (.0230) & .4351 (.0359) && .8148 (.0095) & .7127 (.0142)\\
    Naomi & .6191 (.0322) & .3907 (.0547) && .7917 (.0182) & .6673 (.0290)\\
    Nina  & .6745 (.0245) & .4818 (.0384) && .8263 (.0124) & .7243 (.0188)\\
    Peter & .6687 (.0238) & .4857 (.0293) && .8135 (.0139) & .7092 (.0209)\\
    \hline
  \end{tabular}
  \label{t:framevssub10K}
\end{table}

% \begin{table}[ht]
% \small
% \centering
% \caption{The 70\% of the sentences in the union of 6 child corpora are
%   used as the training set while the remaining 30\% sentences of each
%   corpus are used as the test set.  Average classification accuracy
%   (10 runs) of the supervised connectionist model for the standard
%   labelling on each child corpus after 10K words of training are
%   summarized and the corresponding standard errors are reported in
%   parentheses.}
% \begin{tabular}{|l|l@{ }|l@{ }|l@{ }|l@{ }|l@{ }|l@{ }|l@{ }|}
%   \hline
%   & \multicolumn{4}{c|}{Frames} & \multicolumn{2}{c|}{\specialcell{Number of \\ Substitute Words}}\\
%   \hline
%   Child & aX & Xb & aXb & aX + Xb & 1 & 16\\
%   \hline
%   Anne & .5317 (.0374)  & .5065 (.0214) & .3788 (.0279) & .6190 (.0253) & .6221 (.0333) & .7829 (.0059) \\
%   Aran & .5098 (.0344) & .4779 (.0171) & .5098 (.0289) & .5863 (.0227) & .5916 (.0371) & .7597 (.007) \\
%   Eve & .5408 (.036)  & .4905 (.0183) & .5408 (.0186) & .6125 (.0229) & .6189 (.0287) & .7862 (.0058) \\
%   Naomi & .5250 (.0269) & .4922 (.0205) & .3860 (.028) & .6019 (.0218) & .6021 (.0322) & .7672 (.0071) \\
%   Nina & .5412 (.0304) & .5032 (.0213) & .3950 (.0223) & .6308 (.0277) & .6474 (.0331) & .8089 (.0089) \\
%   Peter & .5359 (.036) & .5092 (.0216) & .5359 (.0188) & .6250 (.0252) & .6315 (.0263) & .7974 (.0082) \\
%   \hline
% \end{tabular}
% \end{table}

%%% Whole results
% \begin{table}[ht]
% \small
% \centering
% \caption{10 fold cross-validation accuracy of the connectionist model on each
%   child corpus with the standard labelling are summarized.  The training
%   phase of each corpus is stopped after 50K word patterns are presented.  Standard
%   errors are reported in parentheses.
% }

% \begin{tabular}{|c|c@{ }|c@{ }|c@{ }|c@{ }|c@{ }|c@{ }|c@{ }|}
%   \hline
%   & \multicolumn{4}{c|}{Frames} & \multicolumn{2}{c|}{\specialcell{Number of \\ Substitute Words}}\\
%   \hline
%   Child & aX & Xb & aXb & aX + Xb & 1 & 16\\
%   \hline
%   Anne & .6165 (.0119)  & .6150 (.0119) & .5251 (.0189) & .7545 (.0147) & .7321 (.0081) & {\bf .8273 (.0087)} \\
%   Aran & .5924 (.0193) & .5583 (.0193) & .4834 (.0131) & .7164 (.0151) & .7081 (.0074) & {\bf .8136 (.0096)} \\
%   Eve & .6337 (.0139)  & .5850 (.0116) & .5425 (.0198) & .7605 (.0104) & .7435 (.0208) & {\bf .8378 (.0199)} \\
%   Naomi & .6183 (.0165) & .5908 (.0254) & .5353 (.0234) & .7438 (.0156) & .7146 (.0136) & {\bf .8165 (.0147)} \\
%   Nina & .6353 (.0195) & .6097 (.0049) & .5556 (.0106) & .7745 (.0199) & .7527 (.0059) & {\bf .8494 (.0073)} \\
%   Peter & .6056 (.0205) & .6232 (.0205) & .5604 (.0115) & .7630 (.005) & .7398 (.0120) & {\bf .8437 (.0061)} \\
%   \hline
% \end{tabular}
% \end{table}
% \begin{table}[h]
% \label{tab:myfirsttable}
% \end{table}


