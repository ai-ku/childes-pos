\section{Related Work}

The previous studies demonstrate that infants have a mechanism to process
statistical properties of natural language. \cite{saffran1996statistical} 
states that 8-month-old infants have a mechanism to process
statistical properties of natural language. Adults also
are sensitive to co-occurrence patterns beyond bigram. In line
with this study, \cite{hahn2012measuring} shows that conditional probabilities
calculated with 4-word contexts are correlated with cloze probabilities.
The cloze probability is a measure of relatedness of a word to a sentences calculated
with EEG signals of the participants. \cite{arnon2010more}
\cite{romberg2010statistical} provides a review of statistical mechanisms
for language acquisition.

Distributional hypothesis or knowledge is a statistical approach to natural
language. The distributional hypothesis suggests that words 
occurring in the similar contexts tend to have similar meaning 
and grammatical properties \cite{harris1954word}. 
\cite{gomez2002variability} and \cite{van2010linking} demonstrate
that distributional statistics help infants acquire non-adjacent
dependencies. \cite{monaghan2012integratingDistributional} proposes that
exploiting distributional regularities of function words is helpful in acquiring word-referent
mappings. Distributional knowledge also plays an 
important part in word segmentation task \cite{saffran1996word}.
\cite{thiessen2012iminerva} claims that an unified model exploiting distributional
statistics may be capable of handling variety of language tasks. Their results on
the tasks of phonetic discrimination, a word learning , and non-adjacent
association learning show that a memory-based approach is successful.

The use of distributional knowledge for syntactic category acquisition
is well studied. \cite{Redington98distributionalinformation} defines the context of
a word as the previous and following words. With this definition, they construct
context vectors of target words for clustering. Using average link clustering
with a threshold maximizing accuracy and completeness, target  words are
separated into categories. Although the categorizations are generally accurate,
the method lacks of completeness. Furthermore, as \cite{ambridge2011child} pointed
out one might question what the underlying process to determine the
threshold for infants to do such clustering could be.

\cite{cartwright1997syntactic} introduces an incremental learning
scheme for syntactic category acquisition. The intrinsic idea behind 
the method is that no language learner is exposed to all the sentences of the language to learn
syntactic categorization. Therefore, distributional knowledge
 may help learner confine the search space for later generalization.
To accomplish this, they convert the syntactic category acquisition into 
Minimum Description Length optimization problem. The crucial part of the method is that after a sentence
is processed, it is forgotten. Their results are similar to Redingtons's work, high
in accuracy but low in completeness. Still, it shows that distributional
knowledge is powerful for syntactic categorization even if the learner is
exposed the small amount of syntactic knowledge of the language.

\cite{Mintz200391} proposes frequent frames. A frame consists of 
two jointly appearing words with one word in the middle. 
Experiments on child directed speech reveal that even a relatively small
fraction of frames has the ability to assign word categories to 
half of the corpora. Though the accuracy is impressive, the same as the previous work, it suffers from
completeness. In addition, it has the coverage problem. \cite{clair2010} 
combines the bigram's coverage power \cite{Redington98distributionalinformation, monaghan2008integration}
 and accuracy of frequent frames \cite{Mintz200391}. Extensive 
experiments result that infants make use of both bigram and 
trigram sources. As they pointed out, they may even use higher-order 
relationships between words. 

\cite{freudenthal2005resolution} points out a different complication on distributional
methods for constructing syntactic categories. One core idea behind
distributional methods is that words occurring in the similar context
 can be used interchangebly. They claim the evaluation methods used in studies
like \cite{Redington98distributionalinformation, monaghan2008integration} or \cite{Mintz200391} 
could be misleading. Specifically, if a word is substituted with another one in its
category, the resulting sentences could be erroneous in a way that they are not observed in
infants speech. As a success criteria, they argue that the proposed categorization
should generate plausible sentences. They introduce chunking mechanism to
overcome this problem by merging words that are seen frequently. Results seems
successful to generate meaningful sentences after substitution, still, the
proposed solution is computationally complex to disclose the learning mechanism
in infants.

More recently, \cite{alishahi2012concurrent} proposes an incremental learning
scheme inducing soft word categories while learning the meaning of words. 
\cite{reeder2013shared} aims to answer the use of distributional knowledge
when the evidence on  the possible context of a word is not enough. Furthermore,
it explains depending on the overlaps between the context words, how and 
when the language users form new categories. They claim that generalization
or restriction of category rules is done in probabilistic manner. \cite{thothathiri2012effect}
examines the role of prosody on infants' distributional learning of
syntactic categories and concludes that the prosody shows little
influence.
