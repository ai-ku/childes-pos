\section{Introduction}
\label{sec:introduction}
%% Describe the problem 
%% Explicitly state the importance 
%% Present result & previous conclusions
%% state the extending points
%% outline purpose
%% list questions and hypotheses
%% state the findings 
%% the contribution
%% outline of the paper
Grammatical rules apply not to individual words (e.g. baby, talk) but to the
grammatical categories (e.g. noun, verb).  Grammatical categories represent the
group of words that can be substituted for one another without altering the
grammatical correctness of a sentence.  Therefore, learning grammatical
categories is an important step in language acquisition.  Researches on
learning grammatical categories were able to show that distributional
information of word co-occurrences is one of the reliable cues
\citep*{Mintz200391,clair2010}[!!  more].  There are also evidences that
lexical stress, prosodic and phonological cues are beneficial in learning
grammatical categories[!!cite]. 

%% \subsection{Comparison with previous distributional approaches}
The distributional representation of word co-occurrences can be grouped into
two: syntagmatic and paradigmatic.  The syntagmatic representation relates
words according to the co-occurrences with the neighboring words while the
paradigmatic representation relates the words that can be substituted for one
another in a given context.

In this paper we hypothesize that infants represent contexts as substitute word
distributions and form grammatical categories accordingly.
Following two examples\footnote{These examples are extracted from the Anne
corpus and substitute word probabilities are calculated as described in
Section~\ref{sec:substitute_vectors}.} illustrate the advantage of paradigmatic
representations in uncovering similarities where no overt similarity that can
be captured by a syntagmatic representation exists. The word ``you'' from the
first sentence and the word ``I'' from the second sentence have no common
neighbors no matter how large the context is.  The paradigmatic representation
captures the similarity of these words by suggesting the similar top
substitutes for both (the numbers in parentheses give substitute
probabilities): 

\begin{quote}
  \small
  % Anne corpus sentence id:13228
  (1) \noindent{\em ``they fall out when {\bf you} put it in the box .''}\\
  \noindent{\bf you:} you(.8188), I(.1027), they(.0408), we(.0146) $\ldots$
\end{quote}

\begin{quote}
  \small
  % Anne corpus sentence id: 13085
  (2) \noindent {\em ``what have {\bf I} got here ?''}\\
  \noindent {\bf I:} we(.8074), you(.1213), I(.0638), they(.0073) $\ldots$
\end{quote}

Note that substitute word distribution of a context ({\it ``fall out when \_
put it in''}) is independent of the actual word (i.e., {\it ``you''}).  The
high probability substitutes reflect both semantic and grammatical properties
of the context.  Top substitutes for ``I'' and ``you'' are not only pro-nouns,
but specifically pro-nouns compatible with the semantic context.  Top
substitutes for the word ``fall'' in the first example consist of words that
are also verbs: come(.7875), go(.0305), fall(.0232), were(.0187) $\ldots$.

These examples shows that the paradigmatic representation relates words according
to the substitute word distribution of their context even when the surface
forms of the contexts do not have any common words.  Thus this makes the
paradigmatic representation more robust to the data sparsity compared to the
syntagmatic representation.

\subsection{Psycholinguistic evidence relevant to substitutes}

\cite{gomez2002variability} showed that 18-months infants are able to detect 

\paragraph {Syntagmatic representation} 
In syntagmatic representation the context is defined with the neighboring
words, typically co-occurrences with a single word on the left or a single word
on the right word called a ``frame'' (e.g., {\em {\bf the} dog {\bf is}; {\bf
the} cat {\bf is}})
\citep*{SchutzePe93,Redington98distributionalinformation, Mintz200391,clair2010,lamar-EtAl:2010:Short,maron2010sphere}. A common limitation of syntagmatic representation is that
it is not possible to exchange information between the words without common
neighbors.  To solve the common neighbor problem one can increase the frame
size however it will introduce the data sparsity due to the low re-occurrence
frequency of large frames \citep*{manning99foundations}.

\cite{Mintz200391} showed that non-adjacent high frequent bi-gram frames,
``aXb'' ({\it a} and {\it b} are the left and the right bigrams, respectively),
are very informative to the language learners on grammatical categorization of
the middle tokens, {\it X}.  The main limitation of this approach is that using
only top-N {\it frequent frames} introduces coverage problem while using all of
the frames introduces frame sparsity.  \cite{clair2010} overcame the
limitations of frequent frames ($aXb$) by introducing {\it flexible frames}
($aX+Xb$) which represent left and right frames separately.  They report
improvements over $aXb$ in terms of accuracy and coverage.  

\paragraph {Paradigmatic representation}

In the paradigmatic representation the context is defined as the distribution
of the substitute words in that context \citep*{SchutzePe93, Schutze1995,
YatbazSY12}.   In part-of-speech induction literature \cite{Schutze1995}
incorporated paradigmatic information by concatenating the left and the right
co-occurrence vectors of the right and left neighbors, respectively and grouped
the words that have similar vectors.  The limitation of \cite{Schutze1995} is
that he uses the bi-gram information and suffers from sparsity as the context
size gets larger.  \cite{YatbazSY12} took a more formal approach and
calculates the most probable substitutes of a given context using a 4-gram
statistical language model.  Their model achieves the state-of-the-art result
in the part-of-speech induction literature.  To the best of our knowledge,
there are not any paradigmatic representation based child language acquisition
models.  


