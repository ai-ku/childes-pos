\section{Introduction}
\label{sec:introduction}
%% Describe the problem 
%% Explicitly state the importance 
%% Present result & previous conclusions
%% state the extending points
%% outline purpose
%% list questions and hypotheses
%% state the findings 
%% the contribution
%% outline of the paper
Grammatical rules apply not to individual words (e.g. baby, talk) but to
grammatical categories (e.g. noun, verb).  Grammatical categories represent the
group of words that can be substituted for one another without altering the
grammatical correctness of a sentence.  Therefore, learning grammatical
categories is an important step in language acquisition.  Studies on
learning grammatical categories have shown that distributional
information of word co-occurrences is one of the reliable cues
\citep*{Mintz200391,clair2010,Redington98distributionalinformation}.  There is
also evidence that lexical stress, prosodic and phonological cues are
beneficial in learning grammatical categories
\citep*{monaghan2012integrating,saffran1996word,saffran1996word}. 

%% \subsection{Comparison with previous distributional approaches}
The distributional representation of word co-occurrences can be grouped into
two: syntagmatic and paradigmatic.  The syntagmatic representation relates
words according to the co-occurrences with the neighboring words while the
paradigmatic representation relates the words that can be substituted for one
another in a given context.

In this paper we hypothesize that infants represent contexts as substitute word
distributions and form grammatical categories accordingly.
The following two examples\footnote{These examples are extracted from the Anne
corpus and substitute word probabilities are calculated as described in
Section~\ref{sec:substitute_vectors}.} illustrate the advantage of paradigmatic
representations in uncovering similarities where no overt similarity that can
be captured by a syntagmatic representation exists. The word ``you'' from the
first sentence and the word ``I'' from the second sentence have no common
neighbors no matter how large the context is.  The paradigmatic representation
captures the similarity of these words by suggesting the similar top
substitutes for both (the numbers in parentheses give substitute
probabilities): 

\begin{quote}
  \small
  % Anne corpus sentence id:13228
  (1) \noindent{\em ``they fall out when {\bf you} put it in the box .''}\\
  \noindent{\bf you:} you(.8188), I(.1027), they(.0408), we(.0146) $\ldots$
\end{quote}

\begin{quote}
  \small
  % Anne corpus sentence id: 13085
  (2) \noindent {\em ``what have {\bf I} got here ?''}\\
  \noindent {\bf I:} we(.8074), you(.1213), I(.0638), they(.0073) $\ldots$
\end{quote}

Note that substitute word distribution of a context ({\it ``fall out when \_
put it in''}) is independent of the actual word (i.e., {\it ``you''}).  The
high probability substitutes reflect both semantic and grammatical properties
of the context.  Top substitutes for ``I'' and ``you'' are not only pronouns,
but specifically pro-nouns compatible with the semantic context.  Top
substitutes for the word ``fall'' in the first example consist of words that
are also verbs: come(.7875), go(.0305), fall(.0232), were(.0187) $\ldots$.

These examples show that the paradigmatic representation relates words according
to the substitute word distribution of their context even when the surface
forms of the contexts do not have any common words.  Thus, this makes the
paradigmatic representation more robust to the data sparsity compared to the
syntagmatic representation.

%\subsection{Psycholinguistic evidence relevant to substitutes}

%\cite{gomez2002variability} showed that 18-months infants are able to detect 

In {\em the syntagmatic representation} the context is defined with the neighboring
words, typically co-occurrences with a single word on the left or a single word
on the right word called a ``frame'' (e.g., {\em {\bf the} dog {\bf is}; {\bf
the} cat {\bf is}}) \citep*{SchutzePe93,Redington98distributionalinformation,
Mintz200391,clair2010,lamar-EtAl:2010:Short,maron2010sphere}. A common
limitation of syntagmatic representation is that it is not possible to exchange
information between the words without common neighbors.  To increase the chance
of having common neighbors the frame size can be increased however this will
introduce the data sparsity due to the low re-occurrence frequency of large
frames \citep*{manning99foundations}.

\cite{Mintz200391} showed that non-adjacent high frequent bi-gram frames,
``aXb'' where {\it a} and {\it b} are the left and the right bigrams,
respectively, are very informative to language learners on grammatical
categorization of the middle tokens, {\it X}.  The main limitation of this
approach is that using only top-N {\it frequent frames} introduces a coverage
problem while using all of the frames introduces frame sparsity.
\cite{clair2010} overcame the limitations of frequent frames ($aXb$) by
introducing {\it flexible frames} ($aX+Xb$) which represent left and right
frames separately.  They report improvements over $aXb$ in terms of accuracy
and coverage.  

In {\em the paradigmatic representation} the context is defined as the distribution
of the substitute words in that context \citep*{SchutzePe93, Schutze1995,
YatbazSY12}.   In part-of-speech induction literature \cite{Schutze1995}
incorporated paradigmatic information by concatenating the left and the right
co-occurrence vectors of the right and left neighbors, respectively and grouped
the words that have similar vectors.  The limitation of this study is that Schutze
uses the bi-gram information and suffers from sparsity as the context
size gets larger.  \cite{YatbazSY12} took a more formal approach and
calculate the most probable substitutes of a given context using a 4-gram
statistical language model.  Their model achieves the state-of-the-art result
in the part-of-speech induction literature.  To the best of our knowledge,
no paradigmatic representation based child language acquisition
models exists.  

In this paper, we introduce a novel representation and hypothesize that
children represent context by constructing substitute word distributions.  To
accomplish this, we adopt the learning framework of \citep*{clair2010} and
compare our new representation with the former syntagmatic based
representations. 

The rest of the paper is organized as follows. In the following section, we explain other
distributional approaches to grammatical category acquisition. Secondly, we provide
a detailed explanation of our calculations of substitute words. In the experiments section,
first we introduce the experimental setup we used and give the details of the experiments
to contrast the paradigmatic approach with the syntagmatic approach. Lastly, we provide a
general discussion on the findings of our results. 
