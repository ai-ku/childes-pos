\section{Substitute Words}
\label{sec:substitute_vectors}

In this study, we predict the syntactic category of a word in a given
context based on its most likely substitute words.  \cite{clair2010}
demonstrated that learning left and right bigrams together was much more
effective than learning them individually.  Thus it is best to use both the
left and the right context when estimating the probabilities for potential
lexical substitutes.  For example, in \emph{``He lived in San Francisco
suburbs.''}, the token \emph{San} would be difficult to guess from the left
context but it is almost certain looking at the right context.  

We define the context $c_w$ of a given word $w$ as the $2n-1$ word window
centered around the position of $w$ : $w_{-n+1} \ldots w \ldots w_{n-1}$.  The
probability of a substitute word $w$ in a given context $c_w$ is estimated
as: 
\begin{eqnarray} 
  \label{eq:lm1}
  P(w_0 = w | c_w) & \propto & P(w_{-n+1}\ldots w_0\ldots w_{n-1})\\
  \label{eq:lm2}& = & P(w_{-n+1})P(w_{-n+2}|w_{-n+1})\ldots P(w_{n-1}|w_{-n+1}^{n-2})\\
  \label{eq:lm3}& \approx & P(w_0| w_{-n+1}^{-1})P(w_{1}|w_{-n+2}^0)\ldots P(w_{n-1}|w^{n-2}) 
\end{eqnarray} 
where $w_i^j$ represents the sequence of words $w_i w_{i+1} \ldots w_{j}$.  In
Equation \ref{eq:lm1}, $P(w|c_w)$ is proportional to $P(w_{-n+1}\ldots w_0
\ldots w_{n+1})$ because the words of the context are fixed.  Terms without
$w_0$ are identical for each substitute in Equation \ref{eq:lm2} therefore they
have been dropped in Equation \ref{eq:lm3}.  Finally, because of the Markov
property of n-gram language model, only the closest $n-1$ words are used in the
experiments.  Note that the substitute word distribution is a function of the
context only and is indifferent to the target word.

Near the sentence boundaries the appropriate terms were truncated in
Equation \ref{eq:lm3}.  Specifically, at the beginning of the sentence
shorter n-gram contexts were used and at the end of the sentence terms
beyond the end-of-sentence utterance were dropped.  

%% Rest of this section details the choice of the data set, the
%% vocabulary and the estimation of substitute probabilities.
%% For computational efficiency only the top 100 substitutes and their
%% unnormalized probabilities were computed for each of the 1,173,766
%% positions in the test set\footnote{The substitutes with unnormalized
%%   log probabilities can be downloaded from
%%   \mbox{\url{http://goo.gl/jzKH0}}.  For a description of the {\sc
%%     fastsubs} algorithm used to generate the substitutes please see
%%   \mbox{\url{http://arxiv.org/abs/1205.5407v1}}.  {\sc fastsubs}
%%   accomplishes this task in about 5 hours, a naive algorithm that
%%   looks at the whole vocabulary would take more than 6 days on a
%%   typical 2012 workstation.}.  The probability vectors for each
%% position were normalized to add up to 1.0 giving us the final
%% substitute vectors used in the rest of this study.

% what is the LM training data
%Train => 5181717 126019973 690121813

