\begin{table}[ht]
  \small 
  \centering
  \caption{10-fold cross-validation classification accuracies of models based
    on flexible frames ($aX + Xb$) and substitutes ($a*b$) on each child corpus
    after 10K training patterns are summarized.  Standard errors are reported
    in parentheses.  Lambdas of $aX+Xb$ and $a*b$ are both tested against each
    other and zero association by using z-test.  All tests have $p<.001$.}
\begin{tabular}{lccccc}
    \hline
    Corpus & \multicolumn{2}{c}{$aX+Xb$} && \multicolumn{2}{c}{$a*b$} \\
    \cline{2-3}
    \cline{5-6}
    & Accuracy & $\lambda$ && Accuracy & $\lambda$\\
    \hline
%% With z-score differences    
%%    & Accuracy & $\lambda$ && Accuracy & $\lambda$ & $\lambda_{a*b}-\lambda_{aX+Xb}$\\
%%    \hline 
    Anne  & .6481 (.1741) & .4712 (.0323) && .8049 (.0059) & .7047 (.0101)\\
    Aran  & .6108 (.0268) & .4238 (.0283) && .7837 (.0071) & .6743 (.0097)\\
    Eve   & .6027 (.0524) & .4275 (.0485) && .8174 (.0110) & .7246 (.0158)\\
    Naomi & .6166 (.0210) & .4160 (.0261) && .7824 (.0137) & .6670 (.0254)\\
    Nina  & .6671 (.0244) & .4878 (.0397) && .8220 (.0055) & .7267 (.0094)\\
    Peter & .6588 (.0226) & .4899 (.0335) && .8152 (.0082) & .7244 (.0119)\\
    \hline
    ALL-TRAIN & & && &\\
    \hline
    Anne  & .6252 (.0231) & .4323 (.0352) && .7970 (.0069) & .6925 (.0111)\\
    Aran  & .5968 (.0218) & .3908 (.0327) && .7783 (.0083) & .6653 (.0123)\\
    Eve   & .6193 (.0192) & .4248 (.0306) && .8091 (.0100) & .7116 (.0141)\\
    Naomi & .6054 (.0236) & .3960 (.0395) && .7771 (.0100) & .6598 (.0178)\\
    Nina  & .6438 (.0216) & .4521 (.0362) && .8146 (.0096) & .7150 (.0162)\\
    Peter & .6255 (.0246) & .4402 (.0372) && .8086 (.0088) & .7140 (.0130)\\
    \hline
  \end{tabular}
  \label{t:framevssub10K}
\end{table}

% \begin{table}[ht]
% \small
% \centering
% \caption{The 70\% of the sentences in the union of 6 child corpora are
%   used as the training set while the remaining 30\% sentences of each
%   corpus are used as the test set.  Average classification accuracy
%   (10 runs) of the supervised connectionist model for the standard
%   labelling on each child corpus after 10K words of training are
%   summarized and the corresponding standard errors are reported in
%   parentheses.}
% \begin{tabular}{|l|l@{ }|l@{ }|l@{ }|l@{ }|l@{ }|l@{ }|l@{ }|}
%   \hline
%   & \multicolumn{4}{c|}{Frames} & \multicolumn{2}{c|}{\specialcell{Number of \\ Substitute Words}}\\
%   \hline
%   Child & aX & Xb & aXb & aX + Xb & 1 & 16\\
%   \hline
%   Anne & .5317 (.0374)  & .5065 (.0214) & .3788 (.0279) & .6190 (.0253) & .6221 (.0333) & .7829 (.0059) \\
%   Aran & .5098 (.0344) & .4779 (.0171) & .5098 (.0289) & .5863 (.0227) & .5916 (.0371) & .7597 (.007) \\
%   Eve & .5408 (.036)  & .4905 (.0183) & .5408 (.0186) & .6125 (.0229) & .6189 (.0287) & .7862 (.0058) \\
%   Naomi & .5250 (.0269) & .4922 (.0205) & .3860 (.028) & .6019 (.0218) & .6021 (.0322) & .7672 (.0071) \\
%   Nina & .5412 (.0304) & .5032 (.0213) & .3950 (.0223) & .6308 (.0277) & .6474 (.0331) & .8089 (.0089) \\
%   Peter & .5359 (.036) & .5092 (.0216) & .5359 (.0188) & .6250 (.0252) & .6315 (.0263) & .7974 (.0082) \\
%   \hline
% \end{tabular}
% \end{table}

%%% Whole results
% \begin{table}[ht]
% \small
% \centering
% \caption{10 fold cross-validation accuracy of the connectionist model on each
%   child corpus with the standard labelling are summarized.  The training
%   phase of each corpus is stopped after 50K word patterns are presented.  Standard
%   errors are reported in parentheses.
% }

% \begin{tabular}{|c|c@{ }|c@{ }|c@{ }|c@{ }|c@{ }|c@{ }|c@{ }|}
%   \hline
%   & \multicolumn{4}{c|}{Frames} & \multicolumn{2}{c|}{\specialcell{Number of \\ Substitute Words}}\\
%   \hline
%   Child & aX & Xb & aXb & aX + Xb & 1 & 16\\
%   \hline
%   Anne & .6165 (.0119)  & .6150 (.0119) & .5251 (.0189) & .7545 (.0147) & .7321 (.0081) & {\bf .8273 (.0087)} \\
%   Aran & .5924 (.0193) & .5583 (.0193) & .4834 (.0131) & .7164 (.0151) & .7081 (.0074) & {\bf .8136 (.0096)} \\
%   Eve & .6337 (.0139)  & .5850 (.0116) & .5425 (.0198) & .7605 (.0104) & .7435 (.0208) & {\bf .8378 (.0199)} \\
%   Naomi & .6183 (.0165) & .5908 (.0254) & .5353 (.0234) & .7438 (.0156) & .7146 (.0136) & {\bf .8165 (.0147)} \\
%   Nina & .6353 (.0195) & .6097 (.0049) & .5556 (.0106) & .7745 (.0199) & .7527 (.0059) & {\bf .8494 (.0073)} \\
%   Peter & .6056 (.0205) & .6232 (.0205) & .5604 (.0115) & .7630 (.005) & .7398 (.0120) & {\bf .8437 (.0061)} \\
%   \hline
% \end{tabular}
% \end{table}
% \begin{table}[h]
% \label{tab:myfirsttable}
% \end{table}


